{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YStfGnlKPS3o"
      },
      "source": [
        "#Basics of Math Behind Neural Networks\n",
        "\n",
        "Neural networks are built on basic math concepts like functions and derivatives. Here we start with a simple quadratic function to show how derivatives measure the slope of a function at any point. This is the foundation for understanding how neural networks learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iknJp4LU8R4K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "020215b5-ee7d-40ac-83d1-983a9fa143d1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approximate derivative at x = 3.0: 14.00003000000538\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define a simple quadratic function\n",
        "def f(x):\n",
        "    return 3*x**2 - 4*x + 5\n",
        "\n",
        "# Generate values for x and compute f(x)\n",
        "xs = np.arange(-5, 5, 0.25)\n",
        "ys = f(xs)\n",
        "\n",
        "# Plot the curve of f(x)\n",
        "plt.plot(xs, ys)\n",
        "plt.title(\"f(x) = 3xÂ² - 4x + 5\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"f(x)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Let's approximate the derivative using a small step (h)\n",
        "h = 0.00001\n",
        "x = 3.0\n",
        "slope = (f(x + h) - f(x)) / h\n",
        "print(f\"Approximate derivative at x = {x}: {slope}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5Un8YY9NFiR"
      },
      "source": [
        "#Finding the Derivative â€” f'(x) = (f(x+h) - f(x)) / h\n",
        "\n",
        "**What does the slope mean?**\n",
        "\n",
        "The slope (or derivative) tells you how fast and in what direction the function value is changing at a specific x.\n",
        "\n",
        "If the slope is **positive**, the function is **increasing** â€” moving upward as x increases.\n",
        "\n",
        "If the slope is **negative**, the function is **decreasing** â€” going downward as x increases.\n",
        "\n",
        "If the slope is 0, the function is flat â€” itâ€™s at a minimum or maximum point.\n",
        "\n",
        "\n",
        "We can estimate it numerically by taking a very small step h. This is called the finite difference method.\n",
        "\n",
        "Letâ€™s calculate it manually and then extend the concept to a small equation with multiple variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj6F8RKQM8Yn",
        "outputId": "7cf68c3c-42c1-4378-94c7-47532b51bc88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14.00003000000538\n"
          ]
        }
      ],
      "source": [
        "h = 0.00001\n",
        "x = 3.0\n",
        "\n",
        "# Approximate derivative of f(x)\n",
        "print((f(x + h) - f(x)) / h)\n",
        "\n",
        "# Using calculus, the exact derivative is:\n",
        "# f'(x) = 6x - 4\n",
        "# f'(3) = 6*3 - 4 = 14\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Getting a Bit More Complex**\n",
        "\n",
        "Now letâ€™s take a simple equation with multiple variables:\n",
        "\n",
        "\n",
        "**d=aâˆ—b+c**\n",
        "\n",
        "Weâ€™ll compute how much d changes when we make a tiny change in a â€” thatâ€™s the derivative of  **d** **with** **respect** **to** **a**\n",
        "\n",
        "So if you see the **slope â‰ˆ 4**, that just means:\n",
        "\n",
        "ðŸ‘‰ At that point, the **curve is going upwards** with a mild slope â€” **for every small step you move in x**, **the function increases by roughly 4Ã— that step**."
      ],
      "metadata": {
        "id": "Jz5c3FlftiSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPRdBroENkRJ",
        "outputId": "d1c8d836-cce3-4c90-a502-75d4c2c1de72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output d: 4.0\n",
            "Derivative with respect to a (slope): -3.000000000419334\n"
          ]
        }
      ],
      "source": [
        "# Inputs\n",
        "a = 2.0\n",
        "b = -3.0\n",
        "c = 10.0\n",
        "\n",
        "# Output of our simple equation\n",
        "d = a * b + c\n",
        "print(\"Output d:\", d)\n",
        "\n",
        "# Small step for numerical derivative\n",
        "h = 0.000001\n",
        "\n",
        "# Compute derivative w.r.t 'a'\n",
        "d1 = a * b + c\n",
        "a += h\n",
        "d2 = a * b + c\n",
        "\n",
        "print(f\"Derivative with respect to a (slope): {(d2 - d1) / h}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "glhNsoBrO3BK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwObcQD8PdGW"
      },
      "source": [
        "#Building the Value Class\n",
        "\n",
        "In deep learning frameworks like PyTorch or TensorFlow, every number involved in computation is tracked automatically â€” not just its value, but also how it was created (from which operations).\n",
        "\n",
        "This allows the framework to later compute gradients automatically (thatâ€™s what backpropagation does).\n",
        "\n",
        "Here, weâ€™ll build our own mini version of that system â€” a class called Value.\n",
        "Each Value object stores:\n",
        "\n",
        "**data**: the actual number\n",
        "\n",
        "**grad**: the gradient (slope) that will be computed later\n",
        "\n",
        "**_op**: the operation used to create it (like +, *, tanh, etc.)\n",
        "\n",
        "**_prev**: the previous values (inputs) used to create it\n",
        "\n",
        "**_backward**: a small function to compute its contribution to the gradient\n",
        "\n",
        "This is the foundation of our tiny autograd engine â€” the same idea used in real neural networks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kK3FOzTPf4W",
        "outputId": "b09966e2-9981-4181-a41a-337e2d60bff7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Value(data=-8.0, grad=0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "\n",
        "class Value:\n",
        "    def __init__(self, data, _children=(), _op='', label=''):\n",
        "        self.data = data            # The actual number\n",
        "        self.grad = 0.0             # The gradient (to be computed)\n",
        "        self._backward = lambda: None   # Function to propagate gradient backward\n",
        "        self._prev = set(_children)     # Children values that led to this value\n",
        "        self._op = _op              # Operation type (+, *, tanh, etc.)\n",
        "        self.label = label          # Label for visualization / debugging\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
        "\n",
        "    # Addition operation\n",
        "    def __add__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data + other.data, (self, other), '+')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += out.grad\n",
        "            other.grad += out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    # Multiplication operation\n",
        "    def __mul__(self, other):\n",
        "        other = other if isinstance(other, Value) else Value(other)\n",
        "        out = Value(self.data * other.data, (self, other), '*')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += other.data * out.grad\n",
        "            other.grad += self.data * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    # Power operation\n",
        "    def __pow__(self, other):\n",
        "        assert isinstance(other, (int, float)), \"only supports int/float powers\"\n",
        "        out = Value(self.data ** other, (self,), f'**{other}')\n",
        "\n",
        "        def _backward():\n",
        "            self.grad += (other * (self.data ** (other - 1))) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    # ReLU activation\n",
        "    def relu(self):\n",
        "        out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
        "        def _backward():\n",
        "            self.grad += (out.data > 0) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    # tanh activation\n",
        "    def tanh(self):\n",
        "        t = math.tanh(self.data)\n",
        "        out = Value(t, (self,), 'tanh')\n",
        "        def _backward():\n",
        "            self.grad += (1 - t**2) * out.grad\n",
        "        out._backward = _backward\n",
        "        return out\n",
        "\n",
        "    # Some helper operations for arithmetic support\n",
        "    def __neg__(self): return self * -1\n",
        "    def __radd__(self, other): return self + other\n",
        "    def __sub__(self, other): return self + (-other)\n",
        "    def __rsub__(self, other): return other + (-self)\n",
        "    def __rmul__(self, other): return self * other\n",
        "    def __truediv__(self, other): return self * other**-1\n",
        "    def __rtruediv__(self, other): return other * self**-1\n",
        "\n",
        "    # Backpropagation\n",
        "    def backward(self):\n",
        "        topo = []\n",
        "        visited = set()\n",
        "\n",
        "        # Build topological order (from inputs to output)\n",
        "        def build_topo(v):\n",
        "            if v not in visited:\n",
        "                visited.add(v)\n",
        "                for child in v._prev:\n",
        "                    build_topo(child)\n",
        "                topo.append(v)\n",
        "        build_topo(self)\n",
        "\n",
        "        # Initialize gradient of the output (self) as 1\n",
        "        self.grad = 1.0\n",
        "\n",
        "        # Propagate gradients backward through all nodes\n",
        "        for node in reversed(topo):\n",
        "            node._backward()\n",
        "\n",
        "# Let's create some simple Values\n",
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(10.0, label='c')\n",
        "\n",
        "# Perform a few operations\n",
        "e = a * b; e.label = 'e'\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label='f')\n",
        "L = d * f; L.label = 'L'\n",
        "\n",
        "# Check the final Value\n",
        "L\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0_Y6_RHtS8r"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ByFkcLvSazz"
      },
      "source": [
        "#for nice visualization\n",
        "\n",
        "When we perform operations like addition or multiplication between Value objects, each step forms a **node** in a computation graph.\n",
        "\n",
        "This graph shows **how data flows** through operations â€” from inputs to the final output.\n",
        "By visualizing it, we can clearly see **how gradients will flow backward during backpropagation**.\n",
        "\n",
        "Below, we use the **graphviz** library to draw this computation graph step by step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wG235HjrQ8Rp"
      },
      "outputs": [],
      "source": [
        "from graphviz import Digraph\n",
        "\n",
        "# Helper function to trace the computation graph\n",
        "def trace(root):\n",
        "    nodes, edges = [], []\n",
        "\n",
        "    def build(v):\n",
        "        if v not in nodes:\n",
        "            nodes.append(v)\n",
        "        for child in v._prev:\n",
        "            edges.append((child, v))\n",
        "            build(child)\n",
        "    build(root)\n",
        "    return nodes, edges\n",
        "\n",
        "# Function to draw the computation graph\n",
        "def draw_dot(root):\n",
        "    dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'})  # Left-to-right layout\n",
        "    nodes, edges = trace(root)\n",
        "\n",
        "    for n in nodes:\n",
        "        uid = str(id(n))\n",
        "        # Each node will display: label, data, and gradient\n",
        "        dot.node(\n",
        "            name=uid,\n",
        "            label=\"{%s | data %.4f | grad %.4f}\" % (n.label, n.data, n.grad),\n",
        "            shape='record'\n",
        "        )\n",
        "\n",
        "        # Show the operation node (like +, *, tanh)\n",
        "        if n._op:\n",
        "            dot.node(name=uid + n._op, label=n._op)\n",
        "            dot.edge(uid + n._op, uid)\n",
        "\n",
        "    # Connect edges between nodes\n",
        "    for n1, n2 in edges:\n",
        "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
        "\n",
        "    return dot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "id": "H8Qp9-DoSg3a",
        "outputId": "a8f1585a-46bc-4903-9595-63f3c4b57cfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"1148pt\" height=\"128pt\"\n viewBox=\"0.00 0.00 1148.00 128.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 124)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-124 1144,-124 1144,4 -4,4\"/>\n<!-- 132018832140784 -->\n<g id=\"node1\" class=\"node\">\n<title>132018832140784</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"948,-27.5 948,-63.5 1140,-63.5 1140,-27.5 948,-27.5\"/>\n<text text-anchor=\"middle\" x=\"960.5\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">L</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"973,-27.5 973,-63.5 \"/>\n<text text-anchor=\"middle\" x=\"1015.5\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;8.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1058,-27.5 1058,-63.5 \"/>\n<text text-anchor=\"middle\" x=\"1099\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018832140784* -->\n<g id=\"node2\" class=\"node\">\n<title>132018832140784*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"885\" cy=\"-45.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"885\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 132018832140784*&#45;&gt;132018832140784 -->\n<g id=\"edge1\" class=\"edge\">\n<title>132018832140784*&#45;&gt;132018832140784</title>\n<path fill=\"none\" stroke=\"black\" d=\"M912.28,-45.5C919.78,-45.5 928.44,-45.5 937.67,-45.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"937.87,-49 947.87,-45.5 937.87,-42 937.87,-49\"/>\n</g>\n<!-- 132018832138240 -->\n<g id=\"node3\" class=\"node\">\n<title>132018832138240</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"635.5,-55.5 635.5,-91.5 820.5,-91.5 820.5,-55.5 635.5,-55.5\"/>\n<text text-anchor=\"middle\" x=\"647\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">d</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"658.5,-55.5 658.5,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"698.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 4.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"738.5,-55.5 738.5,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"779.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018832138240&#45;&gt;132018832140784* -->\n<g id=\"edge4\" class=\"edge\">\n<title>132018832138240&#45;&gt;132018832140784*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M820.51,-56.97C830.48,-55.17 840.13,-53.42 848.77,-51.86\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"849.55,-55.28 858.77,-50.06 848.3,-48.39 849.55,-55.28\"/>\n</g>\n<!-- 132018832138240+ -->\n<g id=\"node4\" class=\"node\">\n<title>132018832138240+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"571\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"571\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 132018832138240+&#45;&gt;132018832138240 -->\n<g id=\"edge2\" class=\"edge\">\n<title>132018832138240+&#45;&gt;132018832138240</title>\n<path fill=\"none\" stroke=\"black\" d=\"M598.29,-73.5C606.26,-73.5 615.54,-73.5 625.39,-73.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"625.41,-77 635.41,-73.5 625.41,-70 625.41,-77\"/>\n</g>\n<!-- 132018786413280 -->\n<g id=\"node5\" class=\"node\">\n<title>132018786413280</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"316,-83.5 316,-119.5 508,-119.5 508,-83.5 316,-83.5\"/>\n<text text-anchor=\"middle\" x=\"327.5\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"339,-83.5 339,-119.5 \"/>\n<text text-anchor=\"middle\" x=\"382.5\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 10.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"426,-83.5 426,-119.5 \"/>\n<text text-anchor=\"middle\" x=\"467\" y=\"-97.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018786413280&#45;&gt;132018832138240+ -->\n<g id=\"edge5\" class=\"edge\">\n<title>132018786413280&#45;&gt;132018832138240+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M508.4,-84.49C517.69,-82.83 526.64,-81.23 534.71,-79.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"535.53,-83.2 544.76,-78 534.31,-76.31 535.53,-83.2\"/>\n</g>\n<!-- 132018832133872 -->\n<g id=\"node6\" class=\"node\">\n<title>132018832133872</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"317,-28.5 317,-64.5 507,-64.5 507,-28.5 317,-28.5\"/>\n<text text-anchor=\"middle\" x=\"328.5\" y=\"-42.8\" font-family=\"Times,serif\" font-size=\"14.00\">e</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"340,-28.5 340,-64.5 \"/>\n<text text-anchor=\"middle\" x=\"382.5\" y=\"-42.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"425,-28.5 425,-64.5 \"/>\n<text text-anchor=\"middle\" x=\"466\" y=\"-42.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018832133872&#45;&gt;132018832138240+ -->\n<g id=\"edge6\" class=\"edge\">\n<title>132018832133872&#45;&gt;132018832138240+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M507.05,-62.67C516.78,-64.35 526.18,-65.96 534.62,-67.42\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"534.3,-70.91 544.75,-69.16 535.49,-64.01 534.3,-70.91\"/>\n</g>\n<!-- 132018832133872* -->\n<g id=\"node7\" class=\"node\">\n<title>132018832133872*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"253\" cy=\"-46.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"253\" y=\"-42.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 132018832133872*&#45;&gt;132018832133872 -->\n<g id=\"edge3\" class=\"edge\">\n<title>132018832133872*&#45;&gt;132018832133872</title>\n<path fill=\"none\" stroke=\"black\" d=\"M280.28,-46.5C288.05,-46.5 297.08,-46.5 306.68,-46.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"306.88,-50 316.88,-46.5 306.88,-43 306.88,-50\"/>\n</g>\n<!-- 132018786402960 -->\n<g id=\"node8\" class=\"node\">\n<title>132018786402960</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-56.5 0,-92.5 190,-92.5 190,-56.5 0,-56.5\"/>\n<text text-anchor=\"middle\" x=\"11.5\" y=\"-70.8\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"23,-56.5 23,-92.5 \"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-70.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"108,-56.5 108,-92.5 \"/>\n<text text-anchor=\"middle\" x=\"149\" y=\"-70.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018786402960&#45;&gt;132018832133872* -->\n<g id=\"edge7\" class=\"edge\">\n<title>132018786402960&#45;&gt;132018832133872*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M190.34,-57.57C199.62,-55.9 208.58,-54.29 216.66,-52.84\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"217.49,-56.25 226.72,-51.04 216.26,-49.36 217.49,-56.25\"/>\n</g>\n<!-- 132018786416736 -->\n<g id=\"node9\" class=\"node\">\n<title>132018786416736</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2.5,-1.5 2.5,-37.5 187.5,-37.5 187.5,-1.5 2.5,-1.5\"/>\n<text text-anchor=\"middle\" x=\"14\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"25.5,-1.5 25.5,-37.5 \"/>\n<text text-anchor=\"middle\" x=\"65.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"105.5,-1.5 105.5,-37.5 \"/>\n<text text-anchor=\"middle\" x=\"146.5\" y=\"-15.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018786416736&#45;&gt;132018832133872* -->\n<g id=\"edge8\" class=\"edge\">\n<title>132018786416736&#45;&gt;132018832133872*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.65,-35.36C197.94,-37.14 207.91,-38.87 216.81,-40.41\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"216.26,-43.87 226.71,-42.12 217.45,-36.97 216.26,-43.87\"/>\n</g>\n<!-- 132018832133968 -->\n<g id=\"node10\" class=\"node\">\n<title>132018832133968</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"634,-0.5 634,-36.5 822,-36.5 822,-0.5 634,-0.5\"/>\n<text text-anchor=\"middle\" x=\"644.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">f</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"655,-0.5 655,-36.5 \"/>\n<text text-anchor=\"middle\" x=\"697.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;2.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"740,-0.5 740,-36.5 \"/>\n<text text-anchor=\"middle\" x=\"781\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018832133968&#45;&gt;132018832140784* -->\n<g id=\"edge9\" class=\"edge\">\n<title>132018832133968&#45;&gt;132018832140784*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M822.29,-34.75C831.57,-36.37 840.52,-37.93 848.61,-39.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"848.22,-42.82 858.68,-41.09 849.42,-35.92 848.22,-42.82\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x78120797fb30>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Let's create some simple Values\n",
        "a = Value(2.0, label='a')\n",
        "b = Value(-3.0, label='b')\n",
        "c = Value(10.0, label='c')\n",
        "\n",
        "# Perform a few operations\n",
        "e = a * b; e.label = 'e'\n",
        "d = e + c; d.label = 'd'\n",
        "f = Value(-2.0, label='f')\n",
        "L = d * f; L.label = 'L'\n",
        "\n",
        "# Check the final Value\n",
        "L\n",
        "draw_dot(L)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMy7yfqfVKzV"
      },
      "source": [
        "#Manually Setting Gradients\n",
        "\n",
        "Before we let our Value class automatically calculate gradients using backpropagation,\n",
        "letâ€™s first understand what those gradients actually mean and how theyâ€™re computed by hand.\n",
        "\n",
        "Weâ€™ll calculate the derivative of the final output L with respect to the input a using the numerical approximation method â€” by making a tiny change h in a and observing how much L changes.\n",
        "\n",
        "This helps us verify that our gradient calculations are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo-vfTY2XnYo",
        "outputId": "d07da928-c415-4488-f95e-e01e1a279b9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Approx derivative of L w.r.t a: 6.000000000128124\n"
          ]
        }
      ],
      "source": [
        "def lol():\n",
        "    h = 0.00001  # a very small step\n",
        "\n",
        "    # First pass - original value of a\n",
        "    a = Value(2.0, label='a')\n",
        "    b = Value(-3.0, label='b')\n",
        "    c = Value(10.0, label='c')\n",
        "    e = a * b; e.label = 'e'\n",
        "    d = e + c; d.label = 'd'\n",
        "    f = Value(-2.0, label='f')\n",
        "    L = d * f; L.label = 'L'\n",
        "    L1 = L.data  # original output\n",
        "\n",
        "    # Second pass - slightly change 'a' by h\n",
        "    a = Value(2.0 + h, label='a')\n",
        "    b = Value(-3.0, label='b')\n",
        "    c = Value(10.0, label='c')\n",
        "    e = a * b; e.label = 'e'\n",
        "    d = e + c; d.label = 'd'\n",
        "    f = Value(-2.0, label='f')\n",
        "    L = d * f; L.label = 'L'\n",
        "    L2 = L.data  # new output\n",
        "\n",
        "    # Numerical derivative (slope)\n",
        "    print(\"Approx derivative of L w.r.t a:\", (L2 - L1) / h)\n",
        "\n",
        "# Run the check\n",
        "lol()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "L.grad = 1     # start from output\n",
        "f.grad = 4     # from d*f â†’ âˆ‚L/âˆ‚f = d\n",
        "d.grad = -2    # âˆ‚L/âˆ‚d = f\n",
        "c.grad = -2    # d = e + c â†’ âˆ‚L/âˆ‚c = d.grad * 1 (chain rule)\n",
        "e.grad = -2    # same as above\n",
        "b.grad = -6    # e = a*b â†’ âˆ‚L/âˆ‚b = a * e.grad\n",
        "a.grad = -6    # âˆ‚L/âˆ‚a = b * e.grad\n"
      ],
      "metadata": {
        "id": "GUtCE5l_ybrl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building a Single Neuron\n",
        "\n",
        "\n",
        "Now that we have our Value class ready, we can simulate a single neuron â€” the fundamental building block of a neural network.\n",
        "\n",
        "A neuron takes some inputs, multiplies them by weights, adds a bias, and then applies an activation function (like tanh, ReLU, or sigmoid) to produce an output.\n",
        "\n",
        "Mathematically:\n",
        "\n",
        "\n",
        "**n= x1w1 + x2w2 + b**\n",
        "\n",
        "\n",
        "**o=tanh(n)**\n",
        "\n",
        "Here:\n",
        "\n",
        "x1, x2 are the inputs\n",
        "\n",
        "w1, w2 are the weights\n",
        "\n",
        "b is the bias\n",
        "\n",
        "o is the output after applying the activation function"
      ],
      "metadata": {
        "id": "d9OYpJsBy_cZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-A5IFEvIgHHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86716d2d-ac19-4a1b-f0e4-59da4fc0f12d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Value(data=0.7071067811865477, grad=0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Inputs to the neuron\n",
        "x1 = Value(2.0, label='x1')\n",
        "x2 = Value(0.0, label='x2')\n",
        "\n",
        "# Weights (these are what weâ€™ll eventually â€œlearnâ€)\n",
        "w1 = Value(-3.0, label='w1')\n",
        "w2 = Value(1.0, label='w2')\n",
        "\n",
        "# Bias term\n",
        "b = Value(6.8813735870195432, label='b')\n",
        "\n",
        "# Linear combination: n = x1*w1 + x2*w2 + b\n",
        "x1w1 = x1 * w1; x1w1.label = 'x1*w1'\n",
        "x2w2 = x2 * w2; x2w2.label = 'x2*w2'\n",
        "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
        "n = x1w1x2w2 + b; n.label = 'n'\n",
        "\n",
        "# Apply tanh activation function\n",
        "o = n.tanh(); o.label = 'o'\n",
        "\n",
        "# Display the final output value\n",
        "o\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "A5_hFOckkrRc"
      },
      "outputs": [],
      "source": [
        "# o.grad=1.0\n",
        "# o._backward()\n",
        "# n._backward()\n",
        "# b._backward()\n",
        "# x1w1x2w2._backward()\n",
        "# x1w1._backward()\n",
        "# x2w2._backward()\n",
        "# w1._backward()\n",
        "# w2._backward()\n",
        "# x1._backward()\n",
        "# x2._backward()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZFkOESAz0Gj_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Backpropagation in Action\n",
        "Backpropagation automatically computes **gradients** of the output with respect to each input, weight, and bias.\n",
        "Gradients tell us **how much each parameter contributes to the output**, which is exactly what we use to **update weights during training**.\n",
        "\n",
        "#What Happens Step by Step:\n",
        "\n",
        "o.backward() traverses the computation graph from the output back to the inputs.\n",
        "\n",
        "It computes the derivative of o with respect to each variable:\n",
        "\n",
        "âˆ‚o/âˆ‚x1\n",
        "\n",
        "âˆ‚o/âˆ‚x2\n",
        "\n",
        "âˆ‚o/âˆ‚w1\n",
        "\n",
        "âˆ‚o/âˆ‚w2\n",
        "\n",
        "âˆ‚o/âˆ‚b\n",
        "\n",
        "The .grad attribute of each Value object is filled with its corresponding gradient.\n",
        "\n",
        "draw_dot(o) gives a visual diagram of the computation graph, showing:\n",
        "\n",
        "Each nodeâ€™s value (data)\n",
        "\n",
        "Each nodeâ€™s gradient (grad)\n",
        "\n",
        "Operations connecting the nodes\n",
        "\n",
        "This helps beginners see exactly how values flow forward and gradients flow backward â€” a very intuitive way to understand neural networks."
      ],
      "metadata": {
        "id": "nDo7e-mh0HK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mRl3hzUXlwsm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "outputId": "4e139ce8-f455-4416-a07e-94ade0722725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"1575pt\" height=\"210pt\"\n viewBox=\"0.00 0.00 1575.00 210.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 206)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-206 1571,-206 1571,4 -4,4\"/>\n<!-- 132018832133152 -->\n<g id=\"node1\" class=\"node\">\n<title>132018832133152</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1382,-54.5 1382,-90.5 1567,-90.5 1567,-54.5 1382,-54.5\"/>\n<text text-anchor=\"middle\" x=\"1393.5\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">o</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1405,-54.5 1405,-90.5 \"/>\n<text text-anchor=\"middle\" x=\"1445\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.7071</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1485,-54.5 1485,-90.5 \"/>\n<text text-anchor=\"middle\" x=\"1526\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.0000</text>\n</g>\n<!-- 132018832133152tanh -->\n<g id=\"node2\" class=\"node\">\n<title>132018832133152tanh</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1319\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1319\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">tanh</text>\n</g>\n<!-- 132018832133152tanh&#45;&gt;132018832133152 -->\n<g id=\"edge1\" class=\"edge\">\n<title>132018832133152tanh&#45;&gt;132018832133152</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1346.04,-72.5C1353.58,-72.5 1362.3,-72.5 1371.57,-72.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1371.81,-76 1381.81,-72.5 1371.81,-69 1371.81,-76\"/>\n</g>\n<!-- 132018832135600 -->\n<g id=\"node3\" class=\"node\">\n<title>132018832135600</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"1071,-54.5 1071,-90.5 1256,-90.5 1256,-54.5 1071,-54.5\"/>\n<text text-anchor=\"middle\" x=\"1082.5\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">n</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1094,-54.5 1094,-90.5 \"/>\n<text text-anchor=\"middle\" x=\"1134\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.8814</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"1174,-54.5 1174,-90.5 \"/>\n<text text-anchor=\"middle\" x=\"1215\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 132018832135600&#45;&gt;132018832133152tanh -->\n<g id=\"edge6\" class=\"edge\">\n<title>132018832135600&#45;&gt;132018832133152tanh</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1256.01,-72.5C1265.01,-72.5 1273.74,-72.5 1281.66,-72.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1281.91,-76 1291.91,-72.5 1281.91,-69 1281.91,-76\"/>\n</g>\n<!-- 132018832135600+ -->\n<g id=\"node4\" class=\"node\">\n<title>132018832135600+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"1008\" cy=\"-72.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"1008\" y=\"-68.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 132018832135600+&#45;&gt;132018832135600 -->\n<g id=\"edge2\" class=\"edge\">\n<title>132018832135600+&#45;&gt;132018832135600</title>\n<path fill=\"none\" stroke=\"black\" d=\"M1035.04,-72.5C1042.58,-72.5 1051.3,-72.5 1060.57,-72.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"1060.81,-76 1070.81,-72.5 1060.81,-69 1060.81,-76\"/>\n</g>\n<!-- 132018832130608 -->\n<g id=\"node5\" class=\"node\">\n<title>132018832130608</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"672,-82.5 672,-118.5 945,-118.5 945,-82.5 672,-82.5\"/>\n<text text-anchor=\"middle\" x=\"725\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">x1*w1 + x2*w2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"778,-82.5 778,-118.5 \"/>\n<text text-anchor=\"middle\" x=\"820.5\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"863,-82.5 863,-118.5 \"/>\n<text text-anchor=\"middle\" x=\"904\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 132018832130608&#45;&gt;132018832135600+ -->\n<g id=\"edge7\" class=\"edge\">\n<title>132018832130608&#45;&gt;132018832135600+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M936.53,-82.49C949.09,-80.71 960.99,-79.02 971.3,-77.56\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"972.03,-80.99 981.44,-76.12 971.05,-74.06 972.03,-80.99\"/>\n</g>\n<!-- 132018832130608+ -->\n<g id=\"node6\" class=\"node\">\n<title>132018832130608+</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"609\" cy=\"-100.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"609\" y=\"-96.8\" font-family=\"Times,serif\" font-size=\"14.00\">+</text>\n</g>\n<!-- 132018832130608+&#45;&gt;132018832130608 -->\n<g id=\"edge3\" class=\"edge\">\n<title>132018832130608+&#45;&gt;132018832130608</title>\n<path fill=\"none\" stroke=\"black\" d=\"M636.23,-100.5C643.7,-100.5 652.41,-100.5 661.87,-100.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"661.98,-104 671.98,-100.5 661.98,-97 661.98,-104\"/>\n</g>\n<!-- 132018832138000 -->\n<g id=\"node7\" class=\"node\">\n<title>132018832138000</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"328.5,-110.5 328.5,-146.5 543.5,-146.5 543.5,-110.5 328.5,-110.5\"/>\n<text text-anchor=\"middle\" x=\"355\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">x2*w2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"381.5,-110.5 381.5,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"421.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"461.5,-110.5 461.5,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"502.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 132018832138000&#45;&gt;132018832130608+ -->\n<g id=\"edge8\" class=\"edge\">\n<title>132018832138000&#45;&gt;132018832130608+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M543.84,-111.01C554.01,-109.34 563.76,-107.74 572.44,-106.32\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"573.17,-109.75 582.47,-104.68 572.04,-102.84 573.17,-109.75\"/>\n</g>\n<!-- 132018832138000* -->\n<g id=\"node8\" class=\"node\">\n<title>132018832138000*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"263\" cy=\"-128.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"263\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 132018832138000*&#45;&gt;132018832138000 -->\n<g id=\"edge4\" class=\"edge\">\n<title>132018832138000*&#45;&gt;132018832138000</title>\n<path fill=\"none\" stroke=\"black\" d=\"M290.34,-128.5C298.51,-128.5 308.08,-128.5 318.36,-128.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"318.39,-132 328.39,-128.5 318.39,-125 318.39,-132\"/>\n</g>\n<!-- 132018831912992 -->\n<g id=\"node9\" class=\"node\">\n<title>132018831912992</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"4,-165.5 4,-201.5 196,-201.5 196,-165.5 4,-165.5\"/>\n<text text-anchor=\"middle\" x=\"19\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">x2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"34,-165.5 34,-201.5 \"/>\n<text text-anchor=\"middle\" x=\"74\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 0.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"114,-165.5 114,-201.5 \"/>\n<text text-anchor=\"middle\" x=\"155\" y=\"-179.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 132018831912992&#45;&gt;132018832138000* -->\n<g id=\"edge9\" class=\"edge\">\n<title>132018831912992&#45;&gt;132018832138000*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M172.53,-165.44C181.84,-162.67 191.2,-159.67 200,-156.5 210.53,-152.71 221.75,-147.9 231.72,-143.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"233.25,-146.48 240.82,-139.07 230.28,-140.14 233.25,-146.48\"/>\n</g>\n<!-- 132018832139824 -->\n<g id=\"node10\" class=\"node\">\n<title>132018832139824</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2.5,-110.5 2.5,-146.5 197.5,-146.5 197.5,-110.5 2.5,-110.5\"/>\n<text text-anchor=\"middle\" x=\"19\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">w2</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"35.5,-110.5 35.5,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 1.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"115.5,-110.5 115.5,-146.5 \"/>\n<text text-anchor=\"middle\" x=\"156.5\" y=\"-124.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.0000</text>\n</g>\n<!-- 132018832139824&#45;&gt;132018832138000* -->\n<g id=\"edge10\" class=\"edge\">\n<title>132018832139824&#45;&gt;132018832138000*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M197.91,-128.5C207.65,-128.5 217.05,-128.5 225.52,-128.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"225.7,-132 235.7,-128.5 225.7,-125 225.7,-132\"/>\n</g>\n<!-- 132018832145872 -->\n<g id=\"node11\" class=\"node\">\n<title>132018832145872</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"326,-55.5 326,-91.5 546,-91.5 546,-55.5 326,-55.5\"/>\n<text text-anchor=\"middle\" x=\"352.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">x1*w1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"379,-55.5 379,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"421.5\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;6.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"464,-55.5 464,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"505\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 132018832145872&#45;&gt;132018832130608+ -->\n<g id=\"edge11\" class=\"edge\">\n<title>132018832145872&#45;&gt;132018832130608+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M546.27,-90.75C555.64,-92.23 564.6,-93.65 572.65,-94.92\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"572.23,-98.4 582.65,-96.5 573.32,-91.48 572.23,-98.4\"/>\n</g>\n<!-- 132018832145872* -->\n<g id=\"node12\" class=\"node\">\n<title>132018832145872*</title>\n<ellipse fill=\"none\" stroke=\"black\" cx=\"263\" cy=\"-73.5\" rx=\"27\" ry=\"18\"/>\n<text text-anchor=\"middle\" x=\"263\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">*</text>\n</g>\n<!-- 132018832145872*&#45;&gt;132018832145872 -->\n<g id=\"edge5\" class=\"edge\">\n<title>132018832145872*&#45;&gt;132018832145872</title>\n<path fill=\"none\" stroke=\"black\" d=\"M290.34,-73.5C297.77,-73.5 306.37,-73.5 315.6,-73.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"315.84,-77 325.84,-73.5 315.84,-70 315.84,-77\"/>\n</g>\n<!-- 132018831901232 -->\n<g id=\"node13\" class=\"node\">\n<title>132018831901232</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"2,-55.5 2,-91.5 198,-91.5 198,-55.5 2,-55.5\"/>\n<text text-anchor=\"middle\" x=\"17\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">x1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"32,-55.5 32,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"72\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 2.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"112,-55.5 112,-91.5 \"/>\n<text text-anchor=\"middle\" x=\"155\" y=\"-69.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad &#45;1.5000</text>\n</g>\n<!-- 132018831901232&#45;&gt;132018832145872* -->\n<g id=\"edge12\" class=\"edge\">\n<title>132018831901232&#45;&gt;132018832145872*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M198.37,-73.5C208.05,-73.5 217.4,-73.5 225.8,-73.5\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"225.91,-77 235.91,-73.5 225.91,-70 225.91,-77\"/>\n</g>\n<!-- 132018833112912 -->\n<g id=\"node14\" class=\"node\">\n<title>132018833112912</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"0,-0.5 0,-36.5 200,-36.5 200,-0.5 0,-0.5\"/>\n<text text-anchor=\"middle\" x=\"16.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">w1</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"33,-0.5 33,-36.5 \"/>\n<text text-anchor=\"middle\" x=\"75.5\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">data &#45;3.0000</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"118,-0.5 118,-36.5 \"/>\n<text text-anchor=\"middle\" x=\"159\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 1.0000</text>\n</g>\n<!-- 132018833112912&#45;&gt;132018832145872* -->\n<g id=\"edge13\" class=\"edge\">\n<title>132018833112912&#45;&gt;132018832145872*</title>\n<path fill=\"none\" stroke=\"black\" d=\"M172.53,-36.56C181.84,-39.33 191.2,-42.33 200,-45.5 210.53,-49.29 221.75,-54.1 231.72,-58.67\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"230.28,-61.86 240.82,-62.93 233.25,-55.52 230.28,-61.86\"/>\n</g>\n<!-- 132018832132192 -->\n<g id=\"node15\" class=\"node\">\n<title>132018832132192</title>\n<polygon fill=\"none\" stroke=\"black\" points=\"716,-27.5 716,-63.5 901,-63.5 901,-27.5 716,-27.5\"/>\n<text text-anchor=\"middle\" x=\"727.5\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"739,-27.5 739,-63.5 \"/>\n<text text-anchor=\"middle\" x=\"779\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">data 6.8814</text>\n<polyline fill=\"none\" stroke=\"black\" points=\"819,-27.5 819,-63.5 \"/>\n<text text-anchor=\"middle\" x=\"860\" y=\"-41.8\" font-family=\"Times,serif\" font-size=\"14.00\">grad 0.5000</text>\n</g>\n<!-- 132018832132192&#45;&gt;132018832135600+ -->\n<g id=\"edge14\" class=\"edge\">\n<title>132018832132192&#45;&gt;132018832135600+</title>\n<path fill=\"none\" stroke=\"black\" d=\"M901.02,-58.01C926,-61.43 951.59,-64.93 971.37,-67.63\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"971.05,-71.12 981.43,-69.01 972,-64.18 971.05,-71.12\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x78120797c110>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Perform backpropagation to compute gradients\n",
        "o.backward()\n",
        "\n",
        "# Visualize the computation graph with gradients\n",
        "draw_dot(o)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "m-QhZ8_vhL5P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or0zMUKlteIc"
      },
      "source": [
        "#Same Computation Using PyTorch\n",
        "PyTorch makes computing forward and backward passes much simpler because it automatically handles gradients for you.\n",
        "\n",
        "ðŸ”¹ **Description**:\n",
        "\n",
        "We define inputs, weights, and bias as tensors with **requires_grad**=**True**.\n",
        "\n",
        "Perform the **forward** **pass** with n = x1*w1 + x2*w2 + b and apply tanh activation.\n",
        "\n",
        "Use **o**.**backward**() to automatically compute gradients for all tensors involved.\n",
        "\n",
        ".**grad** gives the gradient of the output with respect to each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "R80HZ1OltiCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796efd79-42a1-4d00-caa5-fbfbae369274"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7071066904050358\n",
            "---\n",
            "x2 0.5000001283844369\n",
            "x1 -1.5000003851533106\n",
            "w2 0.0\n",
            "w1 1.0000002567688737\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Define inputs\n",
        "x1 = torch.Tensor([2.0]).double(); x1.requires_grad = True\n",
        "x2 = torch.Tensor([0.0]).double(); x2.requires_grad = True\n",
        "\n",
        "# Define weights\n",
        "w1 = torch.Tensor([-3.0]).double(); w1.requires_grad = True\n",
        "w2 = torch.Tensor([1.0]).double(); w2.requires_grad = True\n",
        "\n",
        "# Bias\n",
        "b = torch.Tensor([6.8813735870195432]).double(); b.requires_grad = True\n",
        "\n",
        "# Forward pass\n",
        "n = x1*w1 + x2*w2 + b\n",
        "o = torch.tanh(n)\n",
        "print(\"Output:\", o.data.item())\n",
        "\n",
        "# Backward pass\n",
        "o.backward()\n",
        "\n",
        "# Gradients\n",
        "print('---')\n",
        "print('x1 grad:', x1.grad.item())\n",
        "print('x2 grad:', x2.grad.item())\n",
        "print('w1 grad:', w1.grad.item())\n",
        "print('w2 grad:', w2.grad.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAyIUkH2tUOw"
      },
      "source": [
        "#Building a Deep Neural Network\n",
        "\n",
        "We are creating a mini neural network framework from scratch.\n",
        "\n",
        "**Neuron**: represents a single neuron with weights, bias, and optional ReLU activation.\n",
        "\n",
        "**Layer**: a collection of neurons.\n",
        "\n",
        "**MLP** (**Multi**-**Layer** **Perceptron**): stacks multiple layers to create a deep network.\n",
        "\n",
        "This is a manual version similar to PyTorch but teaches the underlying mechanics of forward passes and parameters.\n",
        "\n",
        "\n",
        "# Simple Explanation :\n",
        "\n",
        "**Neuron** computes a weighted sum of inputs + bias and applies ReLU if non-linear.\n",
        "\n",
        "**Layer** is a collection of neurons. Each neuron processes the same input.\n",
        "\n",
        "**MLP** chains multiple layers together for deep computation.\n",
        "\n",
        "**parameters**() returns all trainable variables (**weights** and **biases**).\n",
        "\n",
        "**This framework allows manual forward pass and gradient computation, teaching the mechanics of deep networks before using frameworks like PyTorch**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBoWOh9TtaEH",
        "outputId": "a7053209-5a4c-49c6-aa2c-e9ba99b90b71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Network output: Value(data=0.0, grad=0.0)\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "class Module:\n",
        "    def zero_grad(self):\n",
        "        for p in self.parameters():\n",
        "            p.grad = 0\n",
        "\n",
        "    def parameters(self):\n",
        "        return []\n",
        "\n",
        "class Neuron(Module):\n",
        "    def __init__(self, nin, nonlin=True):\n",
        "        self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
        "        self.b = Value(0)\n",
        "        self.nonlin = nonlin\n",
        "\n",
        "    def __call__(self, x):\n",
        "        act = sum((wi*xi for wi,xi in zip(self.w, x)), self.b)\n",
        "        return act.relu() if self.nonlin else act\n",
        "\n",
        "    def parameters(self):\n",
        "        return self.w + [self.b]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"{'ReLU' if self.nonlin else 'Linear'}Neuron({len(self.w)})\"\n",
        "\n",
        "class Layer(Module):\n",
        "    def __init__(self, nin, nout, **kwargs):\n",
        "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        out = [n(x) for n in self.neurons]\n",
        "        return out[0] if len(out) == 1 else out\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for n in self.neurons for p in n.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"Layer of [{', '.join(str(n) for n in self.neurons)}]\"\n",
        "\n",
        "class MLP(Module):\n",
        "    def __init__(self, nin, nouts):\n",
        "        sz = [nin] + nouts\n",
        "        self.layers = [Layer(sz[i], sz[i+1], nonlin=i!=len(nouts)-1) for i in range(len(nouts))]\n",
        "\n",
        "    def __call__(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return x\n",
        "\n",
        "    def parameters(self):\n",
        "        return [p for layer in self.layers for p in layer.parameters()]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"MLP of [{', '.join(str(layer) for layer in self.layers)}]\"\n",
        "\n",
        "# Example usage\n",
        "x = [2.0, 3.0, -1.0]  # input vector\n",
        "n = MLP(3, [4, 4, 1]) # MLP: 3 inputs, 3 layers (4, 4, 1 neurons)\n",
        "output = n(x)\n",
        "print(\"Network output:\", output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "F3XANfzq6eSl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Faxm1h2o9Hx9"
      },
      "source": [
        "#Testing Neural Network\n",
        "After building the MLP, we can test it on some sample inputs.\n",
        "\n",
        "**xs** are the input vectors, **ys** are the expected outputs.\n",
        "\n",
        "We feed each input to the network and collect predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3QoTC4fR9LeU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b290dd08-dbac-40b0-fe99-a46fe21d055c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Value(data=-0.9007011574787698, grad=0.0),\n",
              " Value(data=-0.3077048436001332, grad=0.0),\n",
              " Value(data=-0.01629065875677554, grad=0.0),\n",
              " Value(data=-0.5181944678243819, grad=0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Sample inputs\n",
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0],\n",
        "]\n",
        "\n",
        "# Expected outputs\n",
        "ys = [1.0, -1.0, -1.0, 1.0]\n",
        "\n",
        "# Create a new neural network: 3 inputs, 3 layers (4,4,1 neurons)\n",
        "n = MLP(3, [4, 4, 1])\n",
        "\n",
        "# Get predictions\n",
        "ypred = [n(x) for x in xs]\n",
        "ypred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Calculating Loss\n",
        "Loss tells us how far our neural networkâ€™s predictions are from the true values. A simple way to measure this is mean squared error (MSE), which squares the differences between predictions and targets and then averages them.\n",
        "\n",
        "**This gives a single number representing how well the network is performing. Smaller loss means better predictions**"
      ],
      "metadata": {
        "id": "qJ10PYvh22PD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# predictions from the network\n",
        "ypred = [n(x) for x in xs]\n",
        "\n",
        "# calculating mean squared error loss\n",
        "loss = sum((y_p - y_true)**2 for y_p, y_true in zip(ypred, ys)) / len(ys)\n",
        "print(\"Loss:\", loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJNZtnoqElc4",
        "outputId": "2cfcde25-22bc-484a-bb41-e76d18bbcd82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: Value(data=1.841133995949397, grad=0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4HMc6gt3iFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Backpropagation: Reducing Loss and Increasing Accuracy\n",
        "\n"
      ],
      "metadata": {
        "id": "hV8VTJnC3i2j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "vkzBpofDAI2A"
      },
      "outputs": [],
      "source": [
        "# assume we have a simple MLP and computed the loss\n",
        "loss.backward()  # calculate gradients for all parameters\n",
        "\n",
        "# learning rate: how big a step we take\n",
        "lr = 0.01\n",
        "\n",
        "# update each parameter in the network\n",
        "for p in n.parameters():\n",
        "    p.data -= lr * p.grad  # move opposite to the gradient\n",
        "\n",
        "# reset gradients to zero for the next step\n",
        "n.zero_grad()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ewXqpcMaEm56"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop: Improving Accuracy Step by Step\n",
        "\n",
        "Weâ€™ll repeatedly feed our network the inputs, calculate the loss, backpropagate gradients, and update the weights. Over multiple iterations, the network learns and predictions get closer to the true values."
      ],
      "metadata": {
        "id": "QIGBXdmZ35Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "xs = [\n",
        "    [2.0, 3.0, -1.0],\n",
        "    [3.0, -1.0, 0.5],\n",
        "    [0.5, 1.0, 1.0],\n",
        "    [1.0, 1.0, -1.0],\n",
        "]\n",
        "ys = [1.0, -1.0, -1.0, 1.0]\n",
        "\n",
        "# Create a small MLP\n",
        "n = MLP(3, [4, 4, 1])\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.01\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):  # 50 iterations\n",
        "    ypred = [n(x) for x in xs]  # forward pass\n",
        "    # Mean Squared Error Loss\n",
        "    loss = sum((yout - ytrue)**2 for yout, ytrue in zip(ypred, ys)) / len(ys)\n",
        "\n",
        "    # Reset gradients\n",
        "    n.zero_grad()\n",
        "\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Update weights\n",
        "    for p in n.parameters():\n",
        "        p.data -= lr * p.grad\n",
        "\n",
        "    if epoch % 10 == 0:  # print every 10 epochs\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.data:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYJXlyim38xm",
        "outputId": "d95c971d-6894-4eb4-dad2-c544dfad77bc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 1.7849\n",
            "Epoch 10, Loss: 0.9375\n",
            "Epoch 20, Loss: 0.7213\n",
            "Epoch 30, Loss: 0.5456\n",
            "Epoch 40, Loss: 0.3849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGvxKzO33_Ap"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}