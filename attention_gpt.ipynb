{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "r4qsSGIveZB9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jzPCHC07QYzI"
      },
      "outputs": [],
      "source": [
        "with open('/content/attention-gpt-input.txt','r',encoding='utf-8') as f:\n",
        "  text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUfgzHkcQpn-",
        "outputId": "38ba591f-55d2-44eb-ad24-9c1a6807ef23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "len(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "5scNVokI8quB",
        "outputId": "78871651-a375-4c51-dba2-8418afeacfc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars=sorted(list(set(text)))\n",
        "vocab_size=len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YLrxnGsA80Vh",
        "outputId": "10accdc3-1363-4420-892a-6616c3854316"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1: Text Processing & Tokenization\n",
        "\n",
        "**What I implemented:**\n",
        "\n",
        "Calculated the number of unique characters in the text.\n",
        "\n",
        "Created stoi (string-to-index) and itos (index-to-string) dictionaries.\n",
        "\n",
        "Defined encode and decode functions to convert text to numeric sequences and back.\n",
        "\n",
        "**Why this step is important:**\n",
        "\n",
        "Neural networks cannot process raw text, they require numbers.\n",
        "\n",
        "Encoding characters as integers allows the model to learn patterns in sequences.\n",
        "\n",
        "Decoding lets us convert the model’s output back to readable text for evaluation."
      ],
      "metadata": {
        "id": "QytppIhyRUVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)\n",
        "\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmwEULXi9yzV",
        "outputId": "9959b052-0cf6-4a06-e10e-860db89e3502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n",
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data=torch.tensor(encode(text),dtype=torch.long)\n",
        "print(data.shape,data.dtype)\n",
        "print(data[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ahtF8_cAY1O",
        "outputId": "8028246e-bd5a-48e6-dd69-d9ad332fe11c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2: Creating Input-Target Pairs for Training\n",
        "\n",
        "What I implemented:\n",
        "\n",
        "Split the dataset into training (90%) and validation (10%).\n",
        "\n",
        "Defined a block size (sequence length the model sees at once).\n",
        "\n",
        "Created a preview of context-target pairs:\n",
        "\n",
        "context = tokens the model sees\n",
        "\n",
        "target = next token the model should predict\n",
        "\n",
        "Why this step is important:\n",
        "\n",
        "GPT is autoregressive: it predicts the next token based on previous tokens.\n",
        "\n",
        "Generating context-target pairs is how the model learns the sequence structure.\n",
        "\n",
        "Previewing the pairs ensures that batching and sequence slicing are working correctly before training."
      ],
      "metadata": {
        "id": "3ivSy30SSFb1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=int(0.9*len(data))\n",
        "train_data=data[:n]\n",
        "val_data=data[n:]"
      ],
      "metadata": {
        "id": "igXn7EZeA063"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size=8\n",
        "train_data[:block_size+1]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsbGIF-BBPIb",
        "outputId": "55520b29-fdee-4630-fb4d-e0c4be9a004e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=train_data[:block_size]\n",
        "y=train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "  context=x[:t+1]\n",
        "  target=y[t]\n",
        "  print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnG2FEY2CN_A",
        "outputId": "8401d248-9b10-46bf-89f1-a8eec4b611f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3: Creating Mini-Batches\n",
        "What I implemented:\n",
        "\n",
        "Set a batch size (batch_size = 4) and block size (block_size = 8).\n",
        "\n",
        "Created a function get_batch to generate mini-batches for training/validation:\n",
        "\n",
        "Randomly select starting positions in the data\n",
        "\n",
        "Slice sequences of length block_size for inputs (x)\n",
        "\n",
        "Slice the next token for targets (y)\n",
        "\n",
        "Visualized the context-target pairs for every batch and token to verify correctness.\n",
        "\n",
        "Why this step is important:\n",
        "\n",
        "Deep learning models train on batches to speed up computation and stabilize gradient updates.\n",
        "\n",
        "Each batch provides multiple sequences for the model to learn in parallel.\n",
        "\n",
        "Shifting x and y by 1 token ensures the model learns next-token prediction (autoregressive learning).\n",
        "\n",
        "Visualizing context-target pairs helps debug and understand what the model sees."
      ],
      "metadata": {
        "id": "X9YB6hGWSPxz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size=4\n",
        "block_size=8\n",
        "\n",
        "def get_batch(split):\n",
        "  data=train_data if split=='train' else val_data\n",
        "  ix=torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  return x,y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "xb,yb=get_batch('train')\n",
        "print(\"inputs: \")\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print(\"targets:\")\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print(\"----\")\n",
        "\n",
        "for b in range(batch_size):\n",
        "  for t in range(block_size):\n",
        "    context=xb[b,:t+1]\n",
        "    target=yb[b,t]\n",
        "    print(f\"when input is {context.tolist()} the target: {target}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JlDBQHuC0oY",
        "outputId": "916ed137-d666-46fa-d8c2-4ab5cd6c9690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs: \n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zb20Jwu1KLss",
        "outputId": "e97c38c3-c2cd-4126-a04c-61a995671e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
              "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
              "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
              "        [25, 17, 27, 10,  0, 21,  1, 54]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4: Bigram Language Model\n",
        "**What I implemented:**\n",
        "\n",
        "I developed a Bigram language model using a single embedding layer.\n",
        "\n",
        "**Embedding Layer**: The embedding layer learns to map each token (word or character) into a vector space. Each token is associated with a unique vector, which is then used to predict the next token. These embeddings are initially random but get adjusted during training.\n",
        "\n",
        "**Logits**: From the embeddings, the model calculates logits—raw scores that represent the likelihood of each token being the next one in the sequence. These logits are not probabilities yet, but they can be converted to probabilities using the softmax function.\n",
        "\n",
        "**Forward Pass**: In the training loop, the model performs a forward pass to compute the logits for each token. Then, it calculates cross-entropy loss, which measures the difference between the model's predicted token distribution and the actual next token in the sequence.\n",
        "\n",
        "**Text Generation (Autoregressive Sampling)**: To generate new text:\n",
        "\n",
        "The model predicts the next token based on the current context (the sequence of tokens so far).\n",
        "\n",
        "The predicted token is added to the context.\n",
        "\n",
        "This process repeats, and the model continues to generate new tokens until the desired length of the sequence (max_new_tokens) is reached.\n",
        "\n",
        "\n",
        "**Why this step is important:**\n",
        "\n",
        "Serves as a baseline model: captures simple character-to-character dependencies.\n",
        "\n",
        "Helps verify the data pipeline and training loop before building more complex models.\n",
        "\n",
        "Introduces autoregressive generation, the key idea behind GPT: predicting the next token given previous tokens.\n",
        "\n",
        "Shows how softmax + multinomial sampling can produce new text."
      ],
      "metadata": {
        "id": "QrPbCrqiScOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table=nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,idx,targets=None):\n",
        "    logits=self.token_embedding_table(idx)\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      B,T,C=logits.shape\n",
        "      logits=logits.view(B*T,C)\n",
        "      targets=targets.view(B*T)\n",
        "      loss=F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits ,loss\n",
        "\n",
        "  def generate(self,idx,max_new_tokens):\n",
        "    #idx is (B,T) array of the indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "      #get the predictions\n",
        "      logits,loss=self(idx)\n",
        "      # focus only on the last time step\n",
        "      logits=logits[:,-1,:] #becomes (B,C)\n",
        "      probs=F.softmax(logits,dim=-1)#(B,c)\n",
        "      idx_next=torch.multinomial(probs,num_samples=1)#(B,1)\n",
        "      # print(f\"idx_next={idx_next.shape}\")\n",
        "      idx=torch.cat((idx,idx_next),dim=1)#(B,T+1)\n",
        "    return idx\n",
        "\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgeXoqVRLrvf",
        "outputId": "fff3a004-06f8-43e3-a556-d9ac684e4927"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a4f5j0Bj1m82",
        "outputId": "4226ae7b-60d4-4824-82d0-f077599284f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\npJ:Bpm&yiltNCjeO3:Cx&vvMYW-txjuAd IRFbTpJ$zkZelxZtTlHNzdXXUiQQY:qFINTOBNLI,&oTigq z.c:Cq,SDXzetn3XVj'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5: Training the Model\n",
        "\n",
        "**What I implemented:**\n",
        "\n",
        "Initialized the Bigram model and optimizer (AdamW).\n",
        "\n",
        "Wrote a loss evaluation function (estimate_loss) to calculate average training and validation loss without updating model parameters.\n",
        "\n",
        "Implemented the training loop:\n",
        "\n",
        "Every iteration, sample a batch with get_batch\n",
        "\n",
        "Forward pass to compute logits and loss\n",
        "\n",
        "Backpropagation with loss.backward()\n",
        "\n",
        "Update parameters using optimizer.step()\n",
        "\n",
        "Periodically evaluated the model on training and validation data.\n",
        "\n",
        "Printed the final loss and generated sample text to observe learning progress.\n",
        "\n",
        "**Why this step is important:**\n",
        "\n",
        "Training loop is the core of model learning.\n",
        "\n",
        "Evaluating both training and validation loss helps monitor overfitting.\n",
        "\n",
        "Using optimizer.zero_grad() and loss.backward() implements gradient descent, allowing the model to learn from data.\n",
        "\n",
        "Sampling new text after training shows the model’s ability to generate sequences, validating that it learned patterns.\n",
        "\n",
        "This step demonstrates end-to-end training, from data batching to prediction."
      ],
      "metadata": {
        "id": "XRXrK3NpS7HG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "logits, loss = model(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(model.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "optimizer=torch.optim.AdamW(model.parameters(),learning_rate)"
      ],
      "metadata": {
        "id": "Z4nupYSNOuRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses=torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y=get_batch(split)\n",
        "      logits,loss=model(X,Y)\n",
        "      losses[k]=loss.item()\n",
        "    out[split]=losses.mean()\n",
        "  model.train()\n",
        "  return out\n"
      ],
      "metadata": {
        "id": "8aDNljsMEsAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "  if iter%eval_interval==0:\n",
        "    losses=estimate_loss()\n",
        "    print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "  xb,yb=get_batch('train')\n",
        "\n",
        "  logits,loss=model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOZ-uLS4fYW8",
        "outputId": "433a89a2-76b8-4735-eee7-8fc82c06b652"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.7364, val loss 4.7156\n",
            "step 500: train loss 4.3640, val loss 4.3970\n",
            "step 1000: train loss 4.0319, val loss 4.0540\n",
            "step 1500: train loss 3.7849, val loss 3.7783\n",
            "step 2000: train loss 3.5476, val loss 3.5450\n",
            "step 2500: train loss 3.3429, val loss 3.3521\n",
            "step 3000: train loss 3.2083, val loss 3.2211\n",
            "step 3500: train loss 3.0894, val loss 3.0913\n",
            "step 4000: train loss 2.9662, val loss 2.9910\n",
            "step 4500: train loss 2.8842, val loss 2.9064\n",
            "step 5000: train loss 2.8083, val loss 2.8542\n",
            "step 5500: train loss 2.7675, val loss 2.7911\n",
            "step 6000: train loss 2.7200, val loss 2.7098\n",
            "step 6500: train loss 2.6643, val loss 2.6798\n",
            "step 7000: train loss 2.6811, val loss 2.6621\n",
            "step 7500: train loss 2.6318, val loss 2.6400\n",
            "step 8000: train loss 2.5938, val loss 2.6144\n",
            "step 8500: train loss 2.5892, val loss 2.5764\n",
            "step 9000: train loss 2.5758, val loss 2.5973\n",
            "step 9500: train loss 2.5740, val loss 2.5681\n",
            "step 10000: train loss 2.5684, val loss 2.5385\n",
            "step 10500: train loss 2.5405, val loss 2.5269\n",
            "step 11000: train loss 2.5225, val loss 2.5343\n",
            "step 11500: train loss 2.5315, val loss 2.5377\n",
            "step 12000: train loss 2.5076, val loss 2.5431\n",
            "step 12500: train loss 2.4972, val loss 2.5409\n",
            "step 13000: train loss 2.4950, val loss 2.5018\n",
            "step 13500: train loss 2.4813, val loss 2.4678\n",
            "step 14000: train loss 2.5007, val loss 2.5198\n",
            "step 14500: train loss 2.4817, val loss 2.4723\n",
            "step 15000: train loss 2.4510, val loss 2.5173\n",
            "step 15500: train loss 2.4966, val loss 2.4958\n",
            "step 16000: train loss 2.4758, val loss 2.4781\n",
            "step 16500: train loss 2.4878, val loss 2.4836\n",
            "step 17000: train loss 2.4845, val loss 2.4807\n",
            "step 17500: train loss 2.4599, val loss 2.4895\n",
            "step 18000: train loss 2.4614, val loss 2.4974\n",
            "step 18500: train loss 2.4817, val loss 2.4816\n",
            "step 19000: train loss 2.4646, val loss 2.4815\n",
            "step 19500: train loss 2.4669, val loss 2.4492\n",
            "2.519831895828247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx=torch.zeros((1,1),dtype=torch.long)\n",
        "print(decode(model.generate(idx,max_new_tokens=1000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QMOYxj1rz3C",
        "outputId": "dfba13b0-facc-40cf-c27f-e8edefa38835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "e winatllorer lmye\n",
            "Ana dive m s inem e tat f:\n",
            "ALEdnt tha mitrks ban? w.\n",
            "ULAngseou:\n",
            "\n",
            "Wrchumed th pul F s, d moind se is he, ty t bu\n",
            "DWAsullo kinghou at qSuledes cabre velerkearthe I y\n",
            "NARICArey me fr'd nd t GSh pth t y lige n tencoulf ad ded,\n",
            "ARisea ad hean, tagr; mad LEEno ge at ver;\n",
            "wirwn;\n",
            "\n",
            "CHac,\n",
            "Wave as the irn igit d.\n",
            "HNGayave sed a-garesammy sand t s-VINom'sp n:\n",
            "Bothow,\n",
            "Nosathomy.\n",
            "Fur owind yorise biuppee w ls he thout t; is RD ig w an\n",
            "A:\n",
            "tl ane kee eve,\n",
            "Artcke m as h weney, e.\n",
            "CHowes f?\n",
            "\n",
            "Ha a't GHAur.\n",
            "APolld.\n",
            "\n",
            "\n",
            "Ge pas\n",
            "Hathidaw, wilvewounowh lloularoulourithertorer ig'daven'sle,\n",
            "YCHot:\n",
            "Wht Bouredik YOn\n",
            "Tomernorik, an,\n",
            "NGid w'dy; tly ad han,\n",
            "S incke s mokere ifot tis has me:\n",
            "Allcceere, wrery, shasstigsout f verne nre sous, be to e tor othot'Whtit keal tr k,\n",
            "Thin\n",
            "bu.\n",
            "Ant nelads:\n",
            "RYR:\n",
            "\n",
            "ETous tht, illen din'st, wilpr l that!q'sifast d:\n",
            "To brithin dau ns, ak!\n",
            "Whar's-amy; vel whimyothevewhalloler p IO:\n",
            "WI s wak Fld st, thak, w s; adore!\n",
            "A:\n",
            "VO, IAGen Y sthuid la, the, me wshar frrt sh'do \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Why3OA4sHiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6: Understanding the Math Behind Self-Attention\n",
        "What I Implemented:\n",
        "\n",
        "**Lower-Triangular Matrix (a):**\n",
        "I created a lower-triangular matrix a to ensure each token only attends to previous tokens (and not future tokens), following the autoregressive nature of self-attention.\n",
        "\n",
        "**Normalization:**\n",
        "I normalized each row of the matrix a so that the values sum to 1. This step turns the values into attention weights, representing how much each token should attend to other tokens.\n",
        "\n",
        "**Matrix Multiplication (with b):**\n",
        "I multiplied the lower-triangular matrix a with a matrix b (representing token embeddings) to get a new matrix c. This matrix c contains the weighted sum of the token features, where each token’s representation is a combination of the previous tokens, based on the attention weights.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "**Attention as Weighted Sum:** This process demonstrates how self-attention works by calculating a weighted sum of token representations. The weights are determined by the attention mechanism.\n",
        "\n",
        "**Autoregressive Process:** By using the lower-triangular matrix, tokens only attend to previous tokens, ensuring that the model doesn’t cheat by looking ahead in the sequence.\n",
        "\n",
        "**Efficient Computation:** This approach shows how self-attention can be computed using matrix multiplication, a highly efficient method that allows models like Transformers to process long sequences in parallel."
      ],
      "metadata": {
        "id": "HXeMYJrkpD06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MedjSjDCpKiT",
        "outputId": "5ae4cb4b-5daf-47e6-e7fb-7bb45c6a4c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C=4,8,2\n",
        "x=torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVm2nrrgp4re",
        "outputId": "e6f5d3ed-af42-4829-9ffe-99f942f467a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow=torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev=x[b,:t+1]\n",
        "    xbow[b,t]=torch.mean(xprev,0)\n"
      ],
      "metadata": {
        "id": "OlmFzh1zqfwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEllXnSkz1JY",
        "outputId": "4040e32c-d84d-4fa0-d3f8-b4f62c260bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What I Implemented:**\n",
        "\n",
        "I replaced the nested loops with matrix multiplication, allowing me to compute all cumulative averages in a single operation.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "This highlights a key efficiency improvement in self-attention:\n",
        "\n",
        "Rather than iterating over tokens one by one, **matrix multiplication** enables us to compute all weighted sums in parallel with a single operation.\n",
        "\n",
        "This is exactly how **modern Transformer models** handle attention calculations efficiently, leveraging matrix operations to process large sequences quickly."
      ],
      "metadata": {
        "id": "jvHNHNCATyUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#version 2:using matrix multiply for weighted aggregation\n",
        "\n",
        "wei=torch.tril(torch.ones(T,T))\n",
        "print(f\"wei={wei}\")\n",
        "wei=wei/wei.sum(1,keepdim=True)\n",
        "print(f\"wei={wei}\")\n",
        "xbow2=wei@x\n",
        "xbow2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzZIFlsxz5LY",
        "outputId": "2c86852a-16d5-4770-e947-d01ce5f25136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei=tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei=tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]],\n",
              "\n",
              "        [[-0.6631, -0.2513],\n",
              "         [ 0.1735, -0.0649],\n",
              "         [ 0.1685,  0.3348],\n",
              "         [-0.1621,  0.1765],\n",
              "         [-0.2312, -0.0436],\n",
              "         [-0.1015, -0.2855],\n",
              "         [-0.2593, -0.1630],\n",
              "         [-0.3015, -0.2293]],\n",
              "\n",
              "        [[ 1.6455, -0.8030],\n",
              "         [ 1.4985, -0.5395],\n",
              "         [ 0.4954,  0.3420],\n",
              "         [ 1.0623, -0.1802],\n",
              "         [ 1.1401, -0.4462],\n",
              "         [ 1.0870, -0.4071],\n",
              "         [ 1.0430, -0.1299],\n",
              "         [ 1.1138, -0.1641]]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**What I Implemented:**\n",
        "\n",
        "**Masking Future Positions:**\n",
        "I applied a mask to future positions in the attention matrix by setting them to -infinity (-∞). This prevents the model from attending to future tokens when processing the current token, maintaining the autoregressive property.\n",
        "\n",
        "**Softmax Normalization:**\n",
        "I used the softmax function to normalize the attention scores. This converts the raw attention scores (which could be any real number) into a probability distribution. The resulting values are weights that represent how much focus each token should have on the others.\n",
        "\n",
        "**Weighted Aggregation:**\n",
        "After applying softmax, I multiplied the attention weights by the value vectors (which represent the token features). This step computes the final weighted sum of the token features, aggregating information from tokens that the current token is attending to.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "This step demonstrates the core mathematical trick behind self-attention:\n",
        "\n",
        "**Compute Attention Scores:** The first step is to calculate how much attention each token should give to others. This is done through similarity measures (like dot products).\n",
        "\n",
        "**Mask Future Tokens:** We then apply the mask to future tokens to ensure that each token only \"sees\" the tokens before it (and itself), preventing any leakage of future information.\n",
        "\n",
        "**Apply Softmax:** The softmax function turns these attention scores into probabilities, essentially creating attention weights. These weights determine how much influence each token has on the current token’s representation.\n",
        "\n",
        "**Multiply by Value Vectors:** Finally, the attention weights are used to compute a weighted sum of the value vectors, which aggregates the relevant information for each token.\n",
        "\n",
        "This entire process allows each token to focus selectively on the previous tokens in the sequence, gathering relevant context while respecting the order of the sequence. This is essential for tasks like language modeling, where each token needs to make predictions based only on its prior context."
      ],
      "metadata": {
        "id": "hwauhAsDT6GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#version 3:use softmax\n",
        "\n",
        "tril=torch.tril(torch.ones(T,T))\n",
        "print(f\"tril={tril}\")\n",
        "wei=torch.zeros((T,T))\n",
        "wei=wei.masked_fill(tril==0,float('-inf'))\n",
        "print(f\"wei={wei}\")\n",
        "wei=F.softmax(wei,dim=-1)\n",
        "print(wei)\n",
        "wei=wei@x\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYmnUm4j00Jz",
        "outputId": "da60bee9-b060-4b9d-9e97-388f2d98b7d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tril=tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n",
            "wei=tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sgPyNafkIdTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7: Self-Attention Head\n",
        "\n",
        "In this code, we’re implementing a basic **self-attention mechanism**. The goal is to allow each token in a sequence to \"attend\" to all other tokens, including itself, based on the key, query, and value vectors. Here's a simplified breakdown of what's happening in each step:\n",
        "\n",
        "**Key, Query, Value:**\n",
        "\n",
        "**Key**: Encodes information about the token that other tokens will use to decide how much they should \"attend\" to it.\n",
        "\n",
        "**Query**: Represents the current token and how much attention it should give to other tokens.\n",
        "\n",
        "**Value**: Represents the actual information or content that will be passed forward after attention is applied.\n",
        "\n",
        "**Self-Attention:**\n",
        "The self-attention mechanism compares the query of each token to the keys of all tokens (including itself) to calculate an attention score. This score determines how much focus (weight) each token should place on others. The weighted sum of values produces the final output for each token.\n",
        "\n",
        "**Masking Future Tokens:**\n",
        "In this case, the model is performing autoregressive self-attention, so we need to prevent future tokens from influencing the current token’s output. This is done by applying a mask.\n",
        "\n",
        "**Final Output:**\n",
        "After applying the attention mechanism, we get a new representation for each token, which is a weighted sum of the values, based on the attention scores."
      ],
      "metadata": {
        "id": "ueCXbOzUIejU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Define the shape of the input tensor\n",
        "B, T, C = 4, 8, 32  # Batch size, sequence length, feature dimension\n",
        "x = torch.randn(B, T, C)  # Random input tensor with shape (B, T, C)\n",
        "\n",
        "# Define the size of the attention head (output size for each token's attention)\n",
        "head_size = 16\n",
        "\n",
        "# Define the Linear layers to project input x into key, query, and value representations\n",
        "key = nn.Linear(C, head_size, bias=False)  # Projects input into key representation\n",
        "query = nn.Linear(C, head_size, bias=False)  # Projects input into query representation\n",
        "value = nn.Linear(C, head_size, bias=False)  # Projects input into value representation\n",
        "\n",
        "# Compute the key, query, and value tensors by applying the linear layers\n",
        "k = key(x)  # Shape: (B, T, head_size) => (4, 8, 16)\n",
        "q = query(x)  # Shape: (B, T, head_size) => (4, 8, 16)\n",
        "\n",
        "# Compute the attention scores using the dot product between query and key\n",
        "wei = q @ k.transpose(-2, -1)  # Shape: (B, T, T) => (4, 8, 8)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AA3U0osw2GNH",
        "outputId": "e82b9877-1bdd-46f7-b510-d1d9d9043b2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtWZYUXdIRfB",
        "outputId": "dc99b42a-2d81-459b-a7a5-27015b4670be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1571,  0.8801,  0.1615, -0.7824, -0.1429,  0.7468,  0.1007, -0.5239,\n",
              "         -0.8873,  0.1907,  0.1762, -0.5943, -0.4812, -0.4860,  0.2862,  0.5710],\n",
              "        [ 0.6764, -0.5477, -0.2478,  0.3143, -0.1280, -0.2952, -0.4296, -0.1089,\n",
              "         -0.0493,  0.7268,  0.7130, -0.1164,  0.3266,  0.3431, -0.0710,  1.2716],\n",
              "        [ 0.4823, -0.1069, -0.4055,  0.1770,  0.1581, -0.1697,  0.0162,  0.0215,\n",
              "         -0.2490, -0.3773,  0.2787,  0.1629, -0.2895, -0.0676, -0.1416,  1.2194],\n",
              "        [ 0.1971,  0.2856, -0.1303, -0.2655,  0.0668,  0.1954,  0.0281, -0.2451,\n",
              "         -0.4647,  0.0693,  0.1528, -0.2032, -0.2479, -0.1621,  0.1947,  0.7678],\n",
              "        [ 0.2510,  0.7346,  0.5939,  0.2516,  0.2606,  0.7582,  0.5595,  0.3539,\n",
              "         -0.5934, -1.0807, -0.3111, -0.2781, -0.9054,  0.1318, -0.1382,  0.6371],\n",
              "        [ 0.3428,  0.4960,  0.4725,  0.3028,  0.1844,  0.5814,  0.3824,  0.2952,\n",
              "         -0.4897, -0.7705, -0.1172, -0.2541, -0.6892,  0.1979, -0.1513,  0.7666],\n",
              "        [ 0.1866, -0.0964, -0.1430,  0.3059,  0.0834, -0.0069, -0.2047, -0.1535,\n",
              "         -0.0762,  0.3269,  0.3090,  0.0766,  0.0992,  0.1656,  0.1975,  0.7625],\n",
              "        [ 0.1301, -0.0328, -0.4965,  0.2865,  0.2704, -0.2636, -0.0738,  0.3786,\n",
              "          0.0746,  0.0338,  0.0147,  0.3194,  0.2993, -0.1653, -0.0386,  0.3375]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 8: Scaled Dot-Product Attention\n",
        "\n",
        "**What I Implemented:**\n",
        "\n",
        "1.Created **random key and query vectors** for a simple example to simulate attention.\n",
        "\n",
        "2.Computed **attention scores **with the dot product q @ k^T and **scaled** them by dividing by √head_size.\n",
        "\n",
        "3.Applied **causal masking** using a **lower-triangular matrix** to prevent tokens from attending to future positions.\n",
        "\n",
        "4.Used **softmax** to turn the attention scores into **normalized weights** (wei).\n",
        "\n",
        "5.Checked basic statistics (like variance) of keys, queries, and weights, and looked at the first row of attention weights to see how tokens focus on each other.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "**Scaling by √head_size**: Prevents dot products from getting too large, which keeps the softmax stable and gradients well-behaved.\n",
        "\n",
        "**Causal Masking:** Ensures the model respects the autoregressive property—it can only attend to previous tokens, not future ones.\n",
        "\n",
        "**Softmax:** Converts raw attention scores into probabilities, showing how much each token “attends” to others.\n",
        "\n",
        "**Checking Statistics**: Looking at variance and weights helps understand how attention distributes across tokens and can be used for debugging.\n",
        "\n",
        "**In short:** This step demonstrates how scaled dot-product attention computes weighted combinations of tokens, while controlling for numerical stability and respecting the order of sequences."
      ],
      "metadata": {
        "id": "OSdZbciXUhEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k=torch.randn(B,T,head_size)\n",
        "q=torch.randn(B,T,head_size)\n",
        "wei=q@k.transpose(-2,-1)/head_size**0.5\n",
        "tril=torch.tril(torch.ones(T,T))\n",
        "wei=wei.masked_fill(tril==0,float('-inf'))\n",
        "wei=F.softmax(wei,dim=-1)\n",
        "wei.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CB-N8KZkJdKm",
        "outputId": "152beda7-bfe1-4780-9df4-537a1065ff14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 8])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KwVUV5gLeBj",
        "outputId": "c1b6adc1-e414-4720-d729-a8759a6d9a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.9006)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gv3pGB4LrJU",
        "outputId": "d7ab481a-088e-4c28-8806-b5ae31b3b56e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0037)"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-f7UIFcLtD0",
        "outputId": "9fa100da-8465-43d7-cca4-92d6c08eb769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0417)"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-2bvh3xLvbw",
        "outputId": "bafe418d-1f9e-4ed5-cca8-3142eb483a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.6370, 0.3630, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1234, 0.5156, 0.3610, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0348, 0.0261, 0.6100, 0.3292, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2465, 0.3402, 0.0333, 0.3690, 0.0110, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0940, 0.3110, 0.1043, 0.3475, 0.0998, 0.0434, 0.0000, 0.0000],\n",
              "        [0.5549, 0.0228, 0.1602, 0.0460, 0.0605, 0.0840, 0.0717, 0.0000],\n",
              "        [0.0740, 0.0305, 0.1435, 0.1113, 0.4445, 0.0929, 0.0562, 0.0471]])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "syGn3atwMahS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "j6Pfaxs6Pv7X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbJc_YeJmV9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jKDs05I4mWA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "7iYm5VkLmWS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "zE6D2sQQmR6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 9: Implementing a Single Self-Attention Head\n",
        "**What I Implemented:**\n",
        "\n",
        "Created a **PyTorch module** for a single self-attention head.\n",
        "\n",
        "Added **linear layers** to project the input into **key, query, and value vectors**.\n",
        "\n",
        "Computed **scaled dot-product attention**, applied **causal masking**, and **softmax** to get attention weights.\n",
        "\n",
        "Added **dropout** to the attention weights to prevent overfitting.\n",
        "\n",
        "Multiplied the weights by the value vectors to produce **context-aware representations** for each token.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "This is **the core building block **of Transformer models like GPT.\n",
        "\n",
        "Each attention head lets the model focus on **different aspect**s of the previous tokens.\n",
        "\n",
        "Scaling keeps values stable, **masking** preserves autoregressive behavior, and **dropout** helps generalization.\n",
        "\n",
        "Wrapping it in a **module** makes it reusable for **multi-head attention** and full Transformer layers.\n",
        "\n",
        "Core Idea in Simple Terms:\n",
        "A self-attention head allows each token to look at previous tokens, decide which ones are important, and use that information to create a context-aware representation. This is the fundamental mechanism behind how Transformers understand sequences."
      ],
      "metadata": {
        "id": "N6LTaLh5UvMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out"
      ],
      "metadata": {
        "id": "0jhJm9UBPwWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 10: Multi-Head Attention\n",
        "**What I Implemented:**\n",
        "\n",
        "Combined multiple **self-attention heads** in parallel using nn.ModuleList.\n",
        "\n",
        "**Concatenated** the outputs from all heads along the embedding dimension.\n",
        "\n",
        "Applied a **linear projection** (self.proj) to mix information across heads.\n",
        "\n",
        "Added **dropout** for **regularization**.\n",
        "\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "**Multiple Heads**:\n",
        "\n",
        "Each head can focus on different aspects of the sequence—one might capture short-term patterns, another long-range dependencies.\n",
        "\n",
        "**Concatenation + Linear Projection:**\n",
        "\n",
        " Merges the information from all heads into a unified, richer token representation.\n",
        "\n",
        "**Dropout**:\n",
        "Reduces overfitting, especially in large models with many parameters.\n",
        "\n",
        "**Core of Transformer Blocks:**\n",
        " Multi-head attention is essential for Transformers to understand complex relationships in sequences and generate context-aware outputs.\n",
        "\n",
        "**Core Idea in Simple Terms:**\n",
        "\n",
        "Multi-head attention lets the model look at the sequence in multiple ways at once, combine the insights, and produce more powerful token representations, which is what enables Transformers to learn intricate patterns in language or other sequential data."
      ],
      "metadata": {
        "id": "0K2DuDs-VBt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "YN4Hm-mo69aR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 11: FeedForward Layer\n",
        "**What I Implemented:**\n",
        "\n",
        "Built a two-layer **feedforward network** with a **ReLU** **activation** in between.\n",
        "\n",
        "Expanded the embedding dimension from n_embd → 4 * n_embd in the hidden layer, then projected back to n_embd.\n",
        "\n",
        "Added **dropout** for regularization.\n",
        "\n",
        "Applied the network **token-wise**, independently for each position in the sequence.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "**Non-Linearity:** The feedforward layer introduces non-linear transformations, letting the model learn **patterns beyond what attention alone can capture**.\n",
        "\n",
        "**Dimension Expansion:** Expanding the embedding dimension allows the model to **extract richer features** before compressing back to the original size.\n",
        "\n",
        "**Dropout**: Prevents overfitting, especially in large models with many parameters.\n",
        "\n",
        "**Core Part of Transformer Blocks:** Along with multi-head attention and residual connections, the feedforward layer forms a complete Transformer block, which is the fundamental building block of GPT.\n",
        "\n",
        "**Core Idea in Simple Terms:**\n",
        "The feedforward layer lets the model process each token independently to learn complex transformations, making token representations more powerful when combined with attention."
      ],
      "metadata": {
        "id": "dwa40Kr6VMow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "_1COy9fZ0BMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dz9uVV_-VWrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 12: Full Transformer Block\n",
        "**What I Implemented:**\n",
        "\n",
        "**1**.**LayerNorm + Multi-Head Attention + Residual:**\n",
        "\n",
        "Normalize embeddings first (ln1).\n",
        "\n",
        "Apply multi-head attention (sa).\n",
        "\n",
        "Add a residual connection to preserve the original input.\n",
        "\n",
        "2.**LayerNorm + FeedForward + Residual:**\n",
        "\n",
        "Normalize again (ln2).\n",
        "\n",
        "Apply the **feedforward network** (ffwd).\n",
        "\n",
        "Add another **residual connection**.\n",
        "\n",
        "3.Combined these steps into a **single Transformer block**, which can be stacked to build deep Transformer models.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "**Residual Connections:**\n",
        "\n",
        " Help **prevent vanishing gradients** and allow information to flow through deep networks.\n",
        "\n",
        "**Layer Normalization:**\n",
        "\n",
        " Stabilizes training by keeping input distributions consistent.\n",
        "\n",
        "**Multi-Head Attention**:\n",
        "\n",
        " Lets the model aggregate information from multiple tokens in parallel, **capturing complex dependencies**.\n",
        "\n",
        "**FeedForward Layer:**\n",
        "\n",
        " Adds non-linear transformations to enrich token representations.\n",
        "\n",
        "**Stacking Blocks**:\n",
        "\n",
        "Repeating these Transformer blocks forms the backbone of GPT-like architectures, enabling the model to learn complex sequence patterns effectively.\n",
        "\n",
        "**Core Idea in Simple Terms:**\n",
        "A Transformer block combines attention, feedforward transformations, normalization, and residual connections into a single unit. Stacking these blocks lets the model build deep, context-aware representations for sequences, which is the foundation of GPT-like models."
      ],
      "metadata": {
        "id": "VNrUt1R5VXGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "GQcJOEvA12vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 13: Mini GPT Language Model\n",
        "**What I Implemented:**\n",
        "\n",
        "**Token and Positional Embeddings:**\n",
        "\n",
        "Token embeddings convert discrete tokens into continuous vectors.\n",
        "\n",
        "Positional embeddings encode the position of each token so the model knows the order of the sequence.\n",
        "\n",
        "**Stacked Transformer Blocks:**\n",
        "\n",
        "Each block contains multi-head attention, feedforward layers, residual connections, and layer normalization.\n",
        "\n",
        "These blocks build context-aware token representations by combining information from both nearby and distant tokens.\n",
        "\n",
        "**Final LayerNorm:**\n",
        "\n",
        "Stabilizes the output distribution before generating predictions.\n",
        "\n",
        "**Linear Head (lm_head):**\n",
        "\n",
        "Maps the final token representations to logits over the vocabulary, which can be used for prediction.\n",
        "\n",
        "**Forward Pass:**\n",
        "\n",
        "Computes the logits and optionally calculates cross-entropy loss if targets are provided.\n",
        "\n",
        "**Generate Function:**\n",
        "\n",
        "Samples new text autoregressively, predicting one token at a time while respecting the block_size context.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "**Combines All Components:** This step integrates embeddings, attention, feedforward layers, normalization, and residual connections into a full GPT-style language model.\n",
        "\n",
        "**Embeddings**: Make discrete tokens compatible with neural network computations.\n",
        "\n",
        "**Transformer Blocks**: Capture both local and long-range dependencies, allowing the model to understand context.\n",
        "\n",
        "**Residual + LayerNorm**:\n",
        " Ensure stable and deep training, preventing gradient issues.\n",
        "\n",
        "**Autoregressive Generation:**\n",
        " Enables the model to produce coherent sequences starting from a prompt, which is the core of language modeling.\n",
        "\n",
        "**Core Idea in Simple Terms:**\n",
        "\n",
        "A mini GPT takes tokens, encodes their meaning and position, processes them through stacked Transformer blocks to understand context, and predicts the next token step by step. This is how it generates coherent and context-aware text, combining all the building blocks implemented earlier."
      ],
      "metadata": {
        "id": "ed80pkHLVlTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "8aZjpqXlQo4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 14: Training and Generating Text\n",
        "**What I Implemented:**\n",
        "\n",
        "**Setup**:\n",
        "\n",
        "Moved the model to the appropriate device (CPU or GPU) for computation.\n",
        "\n",
        "Counted the total number of parameters to understand model size and complexity.\n",
        "\n",
        "**Optimizer**:\n",
        "\n",
        "Used AdamW, a gradient-based optimizer, to update model weights during training.\n",
        "\n",
        "**Training Loop**:\n",
        "\n",
        "Periodically evaluated loss on training and validation sets to monitor performance.\n",
        "\n",
        "Sampled batches of data using get_batch.\n",
        "\n",
        "Performed a forward pass to compute logits and cross-entropy loss.\n",
        "\n",
        "Backpropagated gradients and updated model weights using the optimizer.\n",
        "\n",
        "**Autoregressive Text Generation:**\n",
        "\n",
        "Started from a prompt (context).\n",
        "\n",
        "Iteratively predicted the next token using the model’s generate method.\n",
        "\n",
        "Converted predicted token indices back into characters to form readable text.\n",
        "\n",
        "**Why This Is Important:**\n",
        "\n",
        "**Training Loop:** Updates the model to minimize prediction loss, enabling it to learn language patterns from the data.\n",
        "\n",
        "**Periodic Evaluation:** Monitors training progress and prevents overfitting.\n",
        "\n",
        "**Autoregressive Generation:** Shows that the model can produce meaningful sequences based on what it has learned.\n",
        "\n",
        "**Parameter Count**: Gives insight into model complexity and the computational resources required.\n",
        "\n",
        "**Core Idea in Simple Terms:**\n",
        "Training adjusts the model so it can predict the next token accurately. Once trained, autoregressive generation lets the model create coherent text from a starting prompt, showing that it has learned meaningful patterns in the data."
      ],
      "metadata": {
        "id": "18qy-1A7VwDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOdCZDyclppQ",
        "outputId": "fb30bf39-47b0-4cdc-fe3d-89477104358d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5058\n",
            "step 300: train loss 2.4194, val loss 2.4334\n",
            "step 400: train loss 2.3501, val loss 2.3568\n",
            "step 500: train loss 2.2963, val loss 2.3129\n",
            "step 600: train loss 2.2410, val loss 2.2501\n",
            "step 700: train loss 2.2057, val loss 2.2191\n",
            "step 800: train loss 2.1633, val loss 2.1860\n",
            "step 900: train loss 2.1242, val loss 2.1498\n",
            "step 1000: train loss 2.1027, val loss 2.1298\n",
            "step 1100: train loss 2.0692, val loss 2.1183\n",
            "step 1200: train loss 2.0386, val loss 2.0797\n",
            "step 1300: train loss 2.0276, val loss 2.0652\n",
            "step 1400: train loss 1.9925, val loss 2.0370\n",
            "step 1500: train loss 1.9702, val loss 2.0302\n",
            "step 1600: train loss 1.9645, val loss 2.0487\n",
            "step 1700: train loss 1.9421, val loss 2.0143\n",
            "step 1800: train loss 1.9091, val loss 1.9953\n",
            "step 1900: train loss 1.9085, val loss 1.9874\n",
            "step 2000: train loss 1.8861, val loss 1.9957\n",
            "step 2100: train loss 1.8731, val loss 1.9765\n",
            "step 2200: train loss 1.8607, val loss 1.9619\n",
            "step 2300: train loss 1.8552, val loss 1.9501\n",
            "step 2400: train loss 1.8410, val loss 1.9419\n",
            "step 2500: train loss 1.8160, val loss 1.9414\n",
            "step 2600: train loss 1.8272, val loss 1.9405\n",
            "step 2700: train loss 1.8118, val loss 1.9348\n",
            "step 2800: train loss 1.8041, val loss 1.9228\n",
            "step 2900: train loss 1.8056, val loss 1.9301\n",
            "step 3000: train loss 1.8001, val loss 1.9244\n",
            "step 3100: train loss 1.7672, val loss 1.9184\n",
            "step 3200: train loss 1.7548, val loss 1.9108\n",
            "step 3300: train loss 1.7586, val loss 1.9065\n",
            "step 3400: train loss 1.7571, val loss 1.8980\n",
            "step 3500: train loss 1.7399, val loss 1.8957\n",
            "step 3600: train loss 1.7283, val loss 1.8927\n",
            "step 3700: train loss 1.7292, val loss 1.8838\n",
            "step 3800: train loss 1.7209, val loss 1.8892\n",
            "step 3900: train loss 1.7222, val loss 1.8685\n",
            "step 4000: train loss 1.7162, val loss 1.8623\n",
            "step 4100: train loss 1.7164, val loss 1.8752\n",
            "step 4200: train loss 1.7036, val loss 1.8638\n",
            "step 4300: train loss 1.7016, val loss 1.8463\n",
            "step 4400: train loss 1.7053, val loss 1.8640\n",
            "step 4500: train loss 1.6885, val loss 1.8471\n",
            "step 4600: train loss 1.6917, val loss 1.8364\n",
            "step 4700: train loss 1.6839, val loss 1.8411\n",
            "step 4800: train loss 1.6672, val loss 1.8399\n",
            "step 4900: train loss 1.6708, val loss 1.8384\n",
            "step 4999: train loss 1.6627, val loss 1.8207\n",
            "\n",
            "FlY BOLINGHARD:\n",
            "Nay, humbract; it contes too\n",
            "must encleming and the second; and say life;\n",
            "In enter all I are and those it;\n",
            "Give out of your I'll tom them nither,\n",
            "One these is news it cy rege;\n",
            "What Naying well and Burryres an fear?\n",
            "\n",
            "OXITVOHN MONFIUS:\n",
            "O is my mily.\n",
            "\n",
            "LEONTES:\n",
            "Geve worman:\n",
            "But guontt not; do spost I vour have well;\n",
            "Not and go the rivisher's become,\n",
            "And alight, upon Crame be with the On man.\n",
            "\n",
            "Roman:\n",
            "What I would and Capolicioual;\n",
            "And wife must he awour,\n",
            "Butcousins the solle with he twomment. Gefore hild you sure\n",
            "That state my not.\n",
            "\n",
            "DUKE OF YORK:\n",
            "My surnt not I have too gentle men\n",
            "Comily comport's that him; I cannot this your\n",
            "house. But as bathol! and now your and;\n",
            "Which-suppy will to coursein to shall her spersend,\n",
            "That you holk all gentled to plartes no mune in en slaicsion,\n",
            "But\n",
            "Thmal, but terruly friend\n",
            "Ristom with the rigess and wilt tentry:\n",
            "I dry that kisspy guase, we mine! crut while with up,\n",
            "I som fries that neish he pray, if,\n",
            "Thom the hre seinged fleby devir begom as goody.\n",
            "Go as thee, thou would may night.\n",
            "\n",
            "ROMEO:\n",
            "It gantle behone, thy lasbeet, him our sitive on;\n",
            "The now to be, all gokss noblambsties. joy to you would do to the woold,\n",
            "Northy will your sould in him, Andrend.\n",
            "\n",
            "LE:\n",
            "My, wense what I will betters, that them end all the sposse is seeess,\n",
            "I Tostry experirts livants you great?\n",
            "I shalk I suort set, for this glied.\n",
            "The some it, men vanty lieht. Murst; or us Volner, still;\n",
            "I wear his crumpurats there suiless Edwift a thoughanted to your ground.\n",
            "Where-be in his is\n",
            "Hard tode toble anoced me the ords,\n",
            "Wonestiful be sweet flough. were you, where 'twon enmer, 'word.\n",
            "\n",
            "POMPEY:\n",
            "Whus bot azy houth this sele yourders?\n",
            "\n",
            "POLFORD NORK:\n",
            "Yet O, sapewer, conted, so, good agion mise thy done\n",
            "on his iffather Befole wefpate,\n",
            "And hrow I teass in I knounged my spite\n",
            "but age so sucalf me with non your\n",
            "and:\n",
            "As one thums of the slive righanneds:\n",
            "Has then that with, and wein that we sterp'd hurse comison toOH!\n",
            "\n",
            "SICHAM:\n",
            "What'm, I have it:\n",
            "Twere I pear news,\n",
            "Twas wha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkXrGAOylDAy",
        "outputId": "c8a01130-b8b2-4918-f872-f22475b9cf75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fireathfend\n",
            "Awhalf arms as your usurn helper\n",
            "To soul Lession\n",
            "Her tale the doth, brother Mothands:\n",
            "Perther, he is compe redion even trundededinue.\n",
            "\n",
            "COMINA:\n",
            "Hot shower:\n",
            "But hy master in creat forth, like you,\n",
            "Hat togetery in plast's divings.\n",
            "\n",
            "RICHARD:\n",
            "Do you love,\n",
            "And my dight use classs?\n",
            "Why, and that soul's make will upity is\n",
            "thou hast, not my flowed sir, he got,\n",
            "Rome that made to hery fool you,---\n",
            "Livet in my basent\n",
            "Yought nour tood some and mastry breive prousiten enuper'd;\n",
            "On confuch leave whileh nemim you.\n",
            "\n",
            "LORD'ELIZhour, this I beling for it. He'll thou,, death flive adot,\n",
            "And for our stries sem with us\n",
            "nevor not toughts such of Rome;\n",
            "When is behone him by his beceence, lenjers.\n",
            "\n",
            "HENRY BOLINGBLIO:\n",
            "My schume, them? whose them his,\n",
            "On lifettimentlend, but your lay us,\n",
            "What therefunds dickngness.\n",
            "But aftunest that wad beek tes woman,\n",
            "My then dout on with a brothed's at that most ove you is banim.\n",
            "\n",
            "First I smun in\n",
            "proyer states heligher beit threaty.\n",
            "\n",
            "BOMVOLIO:\n",
            "Aft? If\n",
            "Far my lose in town, this suchile that you slait, beith sin the seld storry at not.\n",
            "\n",
            "CENRAY:\n",
            "Goy kill min men, you but be From may as\n",
            "primen the Lown to take they more thy bowent Riques\n",
            "That is Wilth sead thum duke some thus, whith an sajust.\n",
            "\n",
            "Let:\n",
            "Love make, at hear\n",
            "That Keemen,\n",
            "Wam dison this livess worg, I she uncount have their two have net now some much my son\n",
            "stage thee my will'st name send the is me bad their wimprend?\n",
            "And--thee desers: slay, thy help tostry.\n",
            "\n",
            "Lovixer:\n",
            "You talt,, if some this retlunged suorfr'ds stray.\n",
            "\n",
            "BUCKINGHAMBE:\n",
            "And thy dislanged imme\n",
            "Bevetity beg! Marwit's behe of you proyalances did the foutrance,\n",
            "Besus stilly nevenders dut him thy here.\n",
            "\n",
            "PERDITHARD:\n",
            "There gold wolter your saja one,\n",
            "to fliful stuch.\n",
            "\n",
            "CORIOLANUS:\n",
            "Ift deart me resperiudle.\n",
            "\n",
            "MENENIUS:\n",
            "O, delting whoy'. Jay'sts-dongetion, tears.\n",
            "\n",
            "ISHAPSING OF GAUUNT:\n",
            "Whyou's thrusbash all trumbtle's all-served\n",
            "But enhold mean befored in senvile hence mes.\n",
            "\n",
            "MARCUTIO:\n",
            "No, lifther, not I mine ime have\n",
            "What his carittlandmist? \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-uBIRgVopRo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}